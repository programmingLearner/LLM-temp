from datasets import load_dataset
from tqdm import tqdm

# Load both datasets
bookcorpus = load_dataset("bookcorpus", split='train')
wikipedia = load_dataset("wikipedia", "20220301.en", split='train')

datasets = [bookcorpus, wikipedia]  # Combine datasets into a list

with DocumentDatabase(reduce_memory=args.reduce_memory) as docs:
    doc_num = 0  # Initialize document number

    # Iterate through each dataset
    for dataset in datasets:
        for entry in tqdm(dataset, desc="Loading Dataset", unit=" docs"):
            text = entry['text'].strip()
            tokens = tokenizer.tokenize(text)
            docs.add_document(tokens)
            
            doc_num += 1
            if doc_num % 100 == 0:
                logger.info('loaded {} docs!'.format(doc_num))

# Add any additional processing or checks here
